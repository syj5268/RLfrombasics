{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole by OpenAI Gym\n",
    "* 카트를 잘 밀어서 막대가 넘어지지 않도록 균형을 잡는 문제 \n",
    "* 액션 : 왼쪽으로 밀기, 오른쪽으로 밀기\n",
    "* 스텝마다 +1 의 보상을 받기 때문에 보상을 최적화하는 것은 오래도록 균형을 잡는 것을 의미함\n",
    "* 카트의 상태 s=(카트의 위치, 카트의 속도, 막대의 각도, 막대의 각속도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 episode : 43 timestep 뒤에 에피소드가 끝났습니다.\n",
      "2 번째 episode : 22 timestep 뒤에 에피소드가 끝났습니다.\n",
      "3 번째 episode : 47 timestep 뒤에 에피소드가 끝났습니다.\n",
      "4 번째 episode : 17 timestep 뒤에 에피소드가 끝났습니다.\n",
      "5 번째 episode : 12 timestep 뒤에 에피소드가 끝났습니다.\n",
      "6 번째 episode : 34 timestep 뒤에 에피소드가 끝났습니다.\n",
      "7 번째 episode : 34 timestep 뒤에 에피소드가 끝났습니다.\n",
      "8 번째 episode : 15 timestep 뒤에 에피소드가 끝났습니다.\n",
      "9 번째 episode : 13 timestep 뒤에 에피소드가 끝났습니다.\n",
      "10 번째 episode : 21 timestep 뒤에 에피소드가 끝났습니다.\n",
      "11 번째 episode : 12 timestep 뒤에 에피소드가 끝났습니다.\n",
      "12 번째 episode : 9 timestep 뒤에 에피소드가 끝났습니다.\n",
      "13 번째 episode : 16 timestep 뒤에 에피소드가 끝났습니다.\n",
      "14 번째 episode : 18 timestep 뒤에 에피소드가 끝났습니다.\n",
      "15 번째 episode : 11 timestep 뒤에 에피소드가 끝났습니다.\n",
      "16 번째 episode : 14 timestep 뒤에 에피소드가 끝났습니다.\n",
      "17 번째 episode : 19 timestep 뒤에 에피소드가 끝났습니다.\n",
      "18 번째 episode : 22 timestep 뒤에 에피소드가 끝났습니다.\n",
      "19 번째 episode : 16 timestep 뒤에 에피소드가 끝났습니다.\n",
      "20 번째 episode : 20 timestep 뒤에 에피소드가 끝났습니다.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#env=gym.make('CartPole-v1', render_mode=\"human\") # 또는 render_mode=\"rgb_array\"\n",
    "env=gym.make('CartPole-v1')\n",
    "\n",
    "for i in range(20):\n",
    "    observation=env.reset()\n",
    "    for t in range(100):\n",
    "        # env.render() # 화면 출력\n",
    "        action= env.action_space.sample() # action을 랜덤으로 선택\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(\"{} 번째 episode : {} timestep 뒤에 에피소드가 끝났습니다.\".format(i+1, t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최신 5만 개의 데이터를 들고 있다가 필요할 때마다 batch_size 만큼의 데이터를 뽑아서 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections # replay buffer을 구현하기 위함 -> deque의 FIFO를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "buffer_limit = 50000\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition): # 데이터를 buffer에 저장\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n): # 버퍼에서 랜덤하게 buffer_size 만큼의 데이터를 뽑아서 미니 배치를 구성해주는 함수\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        # 각각의 데이터를 tensor로 변환\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "            torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "            torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128) # input 차원 : state 4개\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2) # output 차원 : action 2개\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # 마지막 layer에서는 activation function을 사용하지 않음\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon): # epsilon greedy 방식으로 action을 선택\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한 episode가 끝날 때마다 총 320 개의 데이터를 뽑아서 사용\n",
    "* 별도의 Target network를 두었음 : q_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.98\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10): # 10 개의 mini-batch 뽑아서 학습\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s) # input : state\n",
    "        q_a = q_out.gather(1, a) # 실제 선택된 액션의 q값\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask # target q값\n",
    "\n",
    "        # 각 mini-batch마다 loss 계산\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # gradient 계산\n",
    "        optimizer.step() # qnet의 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메인함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.record_video import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "    #env = RecordVideo(env, './video', episode_trigger= lambda episode_number: episode_number%100==0)\n",
    "    #s, _ = env.reset()\n",
    "    #env.start_video_recorder()\n",
    "\n",
    "    # Q network와 Q target network를 생성\n",
    "    q = Qnet()\n",
    "    q_target = Qnet()\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "\n",
    "    # replay buffer 생성\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "    sum_score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "    for n_epi in range(601):\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n",
    "        s, _ = env.reset() ## 수정\n",
    "        done = False\n",
    "\n",
    "        # 데이터 쌓기\n",
    "        while not done:\n",
    "            a = q.sample_action(torch.from_numpy(s).float(), epsilon)\n",
    "            s_prime, r, done, truncated, info = env.step(a) ## 수정\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s, a, r/100.0, s_prime, done_mask)) # reward scaling : 100으로 나눠줌\n",
    "            s = s_prime\n",
    "            score += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        sum_score += score\n",
    "        score = 0.0\n",
    "\n",
    "        # 학습하기\n",
    "        if memory.size() > 2000: # 리플레이 버퍼에 데이터가 충분히 쌓이지 않았을 때 학습을 진행하면 초기의 데이터가 많이 재사용되어 학습이 치우쳐짐\n",
    "            train(q, q_target, memory, optimizer) # episode가 한번 끝날 때마다 train 함수를 호출하여 NN 학습 (q_target network는 업데이트하지 않음)\n",
    "\n",
    "        # 출력하기\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            q_target.load_state_dict(q.state_dict()) # 20번의 episode마다 q_target network를 업데이트\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, buffer size : {}, epsilon : {:.1f}%\"\n",
    "                  .format(n_epi, sum_score/print_interval, memory.size(), epsilon*100))\n",
    "            sum_score = 0.0\n",
    "\n",
    "    #env.close_video_recorder()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 10.4, buffer size : 209, epsilon : 7.9%\n",
      "# of episode :40, avg score : 9.3, buffer size : 395, epsilon : 7.8%\n",
      "# of episode :60, avg score : 9.7, buffer size : 588, epsilon : 7.7%\n",
      "# of episode :80, avg score : 9.9, buffer size : 786, epsilon : 7.6%\n",
      "# of episode :100, avg score : 10.1, buffer size : 987, epsilon : 7.5%\n",
      "# of episode :120, avg score : 9.8, buffer size : 1184, epsilon : 7.4%\n",
      "# of episode :140, avg score : 9.9, buffer size : 1383, epsilon : 7.3%\n",
      "# of episode :160, avg score : 9.5, buffer size : 1573, epsilon : 7.2%\n",
      "# of episode :180, avg score : 9.2, buffer size : 1758, epsilon : 7.1%\n",
      "# of episode :200, avg score : 10.2, buffer size : 1961, epsilon : 7.0%\n",
      "# of episode :220, avg score : 10.1, buffer size : 2162, epsilon : 6.9%\n",
      "# of episode :240, avg score : 9.8, buffer size : 2359, epsilon : 6.8%\n",
      "# of episode :260, avg score : 10.4, buffer size : 2568, epsilon : 6.7%\n",
      "# of episode :280, avg score : 12.9, buffer size : 2827, epsilon : 6.6%\n",
      "# of episode :300, avg score : 14.9, buffer size : 3125, epsilon : 6.5%\n",
      "# of episode :320, avg score : 23.1, buffer size : 3586, epsilon : 6.4%\n",
      "# of episode :340, avg score : 30.9, buffer size : 4205, epsilon : 6.3%\n",
      "# of episode :360, avg score : 15.2, buffer size : 4509, epsilon : 6.2%\n",
      "# of episode :380, avg score : 35.3, buffer size : 5215, epsilon : 6.1%\n",
      "# of episode :400, avg score : 81.3, buffer size : 6841, epsilon : 6.0%\n",
      "# of episode :420, avg score : 181.0, buffer size : 10461, epsilon : 5.9%\n",
      "# of episode :440, avg score : 120.8, buffer size : 12878, epsilon : 5.8%\n",
      "# of episode :460, avg score : 118.4, buffer size : 15246, epsilon : 5.7%\n",
      "# of episode :480, avg score : 147.1, buffer size : 18187, epsilon : 5.6%\n",
      "# of episode :500, avg score : 200.5, buffer size : 22197, epsilon : 5.5%\n",
      "# of episode :520, avg score : 185.8, buffer size : 25914, epsilon : 5.4%\n",
      "# of episode :540, avg score : 198.8, buffer size : 29890, epsilon : 5.3%\n",
      "# of episode :560, avg score : 242.3, buffer size : 34737, epsilon : 5.2%\n",
      "# of episode :580, avg score : 241.7, buffer size : 39570, epsilon : 5.1%\n",
      "# of episode :600, avg score : 234.6, buffer size : 44261, epsilon : 5.0%\n"
     ]
    }
   ],
   "source": [
    "# 결과 1\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 10.2, buffer size : 205, epsilon : 7.9%\n",
      "# of episode :40, avg score : 10.1, buffer size : 407, epsilon : 7.8%\n",
      "# of episode :60, avg score : 9.4, buffer size : 595, epsilon : 7.7%\n",
      "# of episode :80, avg score : 9.8, buffer size : 790, epsilon : 7.6%\n",
      "# of episode :100, avg score : 10.1, buffer size : 991, epsilon : 7.5%\n",
      "# of episode :120, avg score : 10.2, buffer size : 1195, epsilon : 7.4%\n",
      "# of episode :140, avg score : 9.4, buffer size : 1384, epsilon : 7.3%\n",
      "# of episode :160, avg score : 9.7, buffer size : 1577, epsilon : 7.2%\n",
      "# of episode :180, avg score : 9.8, buffer size : 1774, epsilon : 7.1%\n",
      "# of episode :200, avg score : 10.2, buffer size : 1977, epsilon : 7.0%\n",
      "# of episode :220, avg score : 46.1, buffer size : 2899, epsilon : 6.9%\n",
      "# of episode :240, avg score : 43.8, buffer size : 3774, epsilon : 6.8%\n",
      "# of episode :260, avg score : 13.3, buffer size : 4040, epsilon : 6.7%\n",
      "# of episode :280, avg score : 16.6, buffer size : 4373, epsilon : 6.6%\n",
      "# of episode :300, avg score : 11.6, buffer size : 4604, epsilon : 6.5%\n",
      "# of episode :320, avg score : 27.6, buffer size : 5155, epsilon : 6.4%\n",
      "# of episode :340, avg score : 113.5, buffer size : 7426, epsilon : 6.3%\n",
      "# of episode :360, avg score : 124.8, buffer size : 9922, epsilon : 6.2%\n",
      "# of episode :380, avg score : 147.6, buffer size : 12873, epsilon : 6.1%\n",
      "# of episode :400, avg score : 152.9, buffer size : 15931, epsilon : 6.0%\n",
      "# of episode :420, avg score : 177.3, buffer size : 19477, epsilon : 5.9%\n",
      "# of episode :440, avg score : 207.1, buffer size : 23619, epsilon : 5.8%\n",
      "# of episode :460, avg score : 192.2, buffer size : 27462, epsilon : 5.7%\n",
      "# of episode :480, avg score : 253.8, buffer size : 32539, epsilon : 5.6%\n",
      "# of episode :500, avg score : 251.3, buffer size : 37566, epsilon : 5.5%\n",
      "# of episode :520, avg score : 255.3, buffer size : 42672, epsilon : 5.4%\n",
      "# of episode :540, avg score : 196.9, buffer size : 46611, epsilon : 5.3%\n",
      "# of episode :560, avg score : 256.9, buffer size : 50000, epsilon : 5.2%\n",
      "# of episode :580, avg score : 222.2, buffer size : 50000, epsilon : 5.1%\n",
      "# of episode :600, avg score : 279.9, buffer size : 50000, epsilon : 5.0%\n"
     ]
    }
   ],
   "source": [
    "# 결과 2\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 10.5, buffer size : 210, epsilon : 7.9%\n",
      "# of episode :40, avg score : 9.6, buffer size : 402, epsilon : 7.8%\n",
      "# of episode :60, avg score : 9.8, buffer size : 597, epsilon : 7.7%\n",
      "# of episode :80, avg score : 10.2, buffer size : 802, epsilon : 7.6%\n",
      "# of episode :100, avg score : 9.4, buffer size : 990, epsilon : 7.5%\n",
      "# of episode :120, avg score : 9.7, buffer size : 1184, epsilon : 7.4%\n",
      "# of episode :140, avg score : 9.8, buffer size : 1381, epsilon : 7.3%\n",
      "# of episode :160, avg score : 9.7, buffer size : 1574, epsilon : 7.2%\n",
      "# of episode :180, avg score : 9.7, buffer size : 1767, epsilon : 7.1%\n",
      "# of episode :200, avg score : 9.6, buffer size : 1959, epsilon : 7.0%\n",
      "# of episode :220, avg score : 10.5, buffer size : 2169, epsilon : 6.9%\n",
      "# of episode :240, avg score : 33.9, buffer size : 2846, epsilon : 6.8%\n",
      "# of episode :260, avg score : 24.4, buffer size : 3334, epsilon : 6.7%\n",
      "# of episode :280, avg score : 48.0, buffer size : 4295, epsilon : 6.6%\n",
      "# of episode :300, avg score : 144.9, buffer size : 7193, epsilon : 6.5%\n",
      "# of episode :320, avg score : 151.6, buffer size : 10224, epsilon : 6.4%\n",
      "# of episode :340, avg score : 201.3, buffer size : 14250, epsilon : 6.3%\n",
      "# of episode :360, avg score : 254.1, buffer size : 19331, epsilon : 6.2%\n",
      "# of episode :380, avg score : 239.2, buffer size : 24116, epsilon : 6.1%\n",
      "# of episode :400, avg score : 250.5, buffer size : 29126, epsilon : 6.0%\n",
      "# of episode :420, avg score : 386.8, buffer size : 36861, epsilon : 5.9%\n",
      "# of episode :440, avg score : 709.8, buffer size : 50000, epsilon : 5.8%\n",
      "# of episode :460, avg score : 214.6, buffer size : 50000, epsilon : 5.7%\n",
      "# of episode :480, avg score : 299.5, buffer size : 50000, epsilon : 5.6%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_683385/2926016805.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 결과 3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_683385/343932172.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;31m# 데이터 쌓기\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m             \u001b[0ms_prime\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m## 수정\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0mdone_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_683385/1408681533.py\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, obs, epsilon)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# epsilon greedy 방식으로 action을 선택\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mcoin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcoin\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_683385/1408681533.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 마지막 layer에서는 activation function을 사용하지 않음\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1518\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1525\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1526\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1528\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1529\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 결과 3\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비디오 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7faeee74ea70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과를 mp4 동영상으로 보여주기 위한 코드\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find video\n"
     ]
    }
   ],
   "source": [
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
