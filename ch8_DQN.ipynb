{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CartPole by OpenAI Gym\n",
    "* 카트를 잘 밀어서 막대가 넘어지지 않도록 균형을 잡는 문제 \n",
    "* 액션 : 왼쪽으로 밀기, 오른쪽으로 밀기\n",
    "* 스텝마다 +1 의 보상을 받기 때문에 보상을 최적화하는 것은 오래도록 균형을 잡는 것을 의미함\n",
    "* 카트의 상태 s=(카트의 위치, 카트의 속도, 막대의 각도, 막대의 각속도)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 번째 episode : 24 timestep 뒤에 에피소드가 끝났습니다.\n",
      "2 번째 episode : 63 timestep 뒤에 에피소드가 끝났습니다.\n",
      "3 번째 episode : 22 timestep 뒤에 에피소드가 끝났습니다.\n",
      "4 번째 episode : 32 timestep 뒤에 에피소드가 끝났습니다.\n",
      "5 번째 episode : 14 timestep 뒤에 에피소드가 끝났습니다.\n",
      "6 번째 episode : 10 timestep 뒤에 에피소드가 끝났습니다.\n",
      "7 번째 episode : 48 timestep 뒤에 에피소드가 끝났습니다.\n",
      "8 번째 episode : 18 timestep 뒤에 에피소드가 끝났습니다.\n",
      "9 번째 episode : 26 timestep 뒤에 에피소드가 끝났습니다.\n",
      "10 번째 episode : 85 timestep 뒤에 에피소드가 끝났습니다.\n",
      "11 번째 episode : 28 timestep 뒤에 에피소드가 끝났습니다.\n",
      "12 번째 episode : 13 timestep 뒤에 에피소드가 끝났습니다.\n",
      "13 번째 episode : 35 timestep 뒤에 에피소드가 끝났습니다.\n",
      "14 번째 episode : 24 timestep 뒤에 에피소드가 끝났습니다.\n",
      "15 번째 episode : 12 timestep 뒤에 에피소드가 끝났습니다.\n",
      "16 번째 episode : 11 timestep 뒤에 에피소드가 끝났습니다.\n",
      "17 번째 episode : 21 timestep 뒤에 에피소드가 끝났습니다.\n",
      "18 번째 episode : 33 timestep 뒤에 에피소드가 끝났습니다.\n",
      "19 번째 episode : 34 timestep 뒤에 에피소드가 끝났습니다.\n",
      "20 번째 episode : 13 timestep 뒤에 에피소드가 끝났습니다.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "#env=gym.make('CartPole-v1', render_mode=\"human\") # 또는 render_mode=\"rgb_array\"\n",
    "env=gym.make('CartPole-v1')\n",
    "\n",
    "for i in range(20):\n",
    "    observation=env.reset()\n",
    "    for t in range(100):\n",
    "        # env.render() # 화면 출력\n",
    "        action= env.action_space.sample() # action을 랜덤으로 선택\n",
    "        observation, reward, done, truncated, info = env.step(action)\n",
    "        \n",
    "        if done:\n",
    "            print(\"{} 번째 episode : {} timestep 뒤에 에피소드가 끝났습니다.\".format(i+1, t+1))\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replay Buffer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최신 5만 개의 데이터를 들고 있다가 필요할 때마다 batch_size 만큼의 데이터를 뽑아서 제공"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections # replay buffer을 구현하기 위함 -> deque의 FIFO를 이용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "buffer_limit = 50000\n",
    "\n",
    "class ReplayBuffer():\n",
    "    def __init__(self):\n",
    "        self.buffer = collections.deque(maxlen=buffer_limit)\n",
    "\n",
    "    def put(self, transition): # 데이터를 buffer에 저장\n",
    "        self.buffer.append(transition)\n",
    "\n",
    "    def sample(self, n): # 버퍼에서 랜덤하게 buffer_size 만큼의 데이터를 뽑아서 미니 배치를 구성해주는 함수\n",
    "        mini_batch = random.sample(self.buffer, n)\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_mask_lst = [], [], [], [], []\n",
    "        for transition in mini_batch:\n",
    "            s, a, r, s_prime, done_mask = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask_lst.append([done_mask])\n",
    "        \n",
    "        # 각각의 데이터를 tensor로 변환\n",
    "        return torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "            torch.tensor(r_lst), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "            torch.tensor(done_mask_lst)\n",
    "    \n",
    "    def size(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 에이전트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Qnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Qnet, self).__init__()\n",
    "        self.fc1 = nn.Linear(4, 128) # input 차원 : state 4개\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 2) # output 차원 : action 2개\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x) # 마지막 layer에서는 activation function을 사용하지 않음\n",
    "        return x\n",
    "    \n",
    "    def sample_action(self, obs, epsilon): # epsilon greedy 방식으로 action을 선택\n",
    "        out = self.forward(obs)\n",
    "        coin = random.random()\n",
    "        if coin < epsilon:\n",
    "            return random.randint(0, 1)\n",
    "        else:\n",
    "            return out.argmax().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 학습함수"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 한 episode가 끝날 때마다 총 320 개의 데이터를 뽑아서 사용\n",
    "* 별도의 Target network를 두었음 : q_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "gamma = 0.98\n",
    "batch_size=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(q, q_target, memory, optimizer):\n",
    "    for i in range(10): # 10 개의 mini-batch 뽑아서 학습\n",
    "        s, a, r, s_prime, done_mask = memory.sample(batch_size)\n",
    "\n",
    "        q_out = q(s) # input : state\n",
    "        q_a = q_out.gather(1, a) # 실제 선택된 액션의 q값\n",
    "        max_q_prime = q_target(s_prime).max(1)[0].unsqueeze(1)\n",
    "        target = r + gamma * max_q_prime * done_mask # target q값\n",
    "\n",
    "        # 각 mini-batch마다 loss 계산\n",
    "        loss = F.smooth_l1_loss(q_a, target)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # gradient 계산\n",
    "        optimizer.step() # qnet의 파라미터 업데이트"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 메인함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "learning_rate = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers.record_video import RecordVideo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v1', render_mode=\"rgb_array\")\n",
    "\n",
    "    #env = RecordVideo(env, './video', episode_trigger= lambda episode_number: episode_number%100==0)\n",
    "    #s, _ = env.reset()\n",
    "    #env.start_video_recorder()\n",
    "\n",
    "    # Q network와 Q target network를 생성\n",
    "    q = Qnet()\n",
    "    q_target = Qnet()\n",
    "    q_target.load_state_dict(q.state_dict())\n",
    "\n",
    "    # replay buffer 생성\n",
    "    memory = ReplayBuffer()\n",
    "\n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "    sum_score = 0.0\n",
    "    optimizer = optim.Adam(q.parameters(), lr=learning_rate)\n",
    "\n",
    "    for n_epi in range(600):\n",
    "        epsilon = max(0.01, 0.08 - 0.01*(n_epi/200)) # Linear annealing from 8% to 1%\n",
    "        s, _ = env.reset() ## 수정\n",
    "        done = False\n",
    "\n",
    "        # 데이터 쌓기\n",
    "        while not done:\n",
    "            a = q.sample_action(torch.from_numpy(s).float(), epsilon)\n",
    "            s_prime, r, done, truncated, info = env.step(a) ## 수정\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            memory.put((s, a, r/100.0, s_prime, done_mask)) # reward scaling : 100으로 나눠줌\n",
    "            s = s_prime\n",
    "            score += r\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "            if score > 50000: \n",
    "                print(\"Wow! Score is over 50000!\")\n",
    "            \n",
    "        sum_score += score\n",
    "        score = 0.0\n",
    "\n",
    "        # 학습하기\n",
    "        if memory.size() > 2000: # 리플레이 버퍼에 데이터가 충분히 쌓이지 않았을 때 학습을 진행하면 초기의 데이터가 많이 재사용되어 학습이 치우쳐짐\n",
    "            train(q, q_target, memory, optimizer) # episode가 한번 끝날 때마다 train 함수를 호출하여 NN 학습 (q_target network는 업데이트하지 않음)\n",
    "\n",
    "        # 출력하기\n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            q_target.load_state_dict(q.state_dict()) # 20번의 episode마다 q_target network를 업데이트\n",
    "            print(\"# of episode :{}, avg score : {:.1f}, buffer size : {}, epsilon : {:.1f}%\"\n",
    "                  .format(n_epi, sum_score/print_interval, memory.size(), epsilon*100))\n",
    "            sum_score = 0.0\n",
    "\n",
    "    #env.close_video_recorder()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 42.4, buffer size : 848, epsilon : 7.9%\n",
      "# of episode :40, avg score : 43.8, buffer size : 1724, epsilon : 7.8%\n",
      "# of episode :60, avg score : 24.6, buffer size : 2216, epsilon : 7.7%\n",
      "# of episode :80, avg score : 13.8, buffer size : 2492, epsilon : 7.6%\n",
      "# of episode :100, avg score : 11.8, buffer size : 2728, epsilon : 7.5%\n",
      "# of episode :120, avg score : 40.1, buffer size : 3531, epsilon : 7.4%\n",
      "# of episode :140, avg score : 24.5, buffer size : 4021, epsilon : 7.3%\n",
      "# of episode :160, avg score : 15.1, buffer size : 4322, epsilon : 7.2%\n",
      "# of episode :180, avg score : 12.8, buffer size : 4578, epsilon : 7.1%\n",
      "# of episode :200, avg score : 10.3, buffer size : 4784, epsilon : 7.0%\n",
      "# of episode :220, avg score : 12.2, buffer size : 5029, epsilon : 6.9%\n",
      "# of episode :240, avg score : 31.6, buffer size : 5660, epsilon : 6.8%\n",
      "# of episode :260, avg score : 88.6, buffer size : 7432, epsilon : 6.7%\n",
      "# of episode :280, avg score : 149.4, buffer size : 10420, epsilon : 6.6%\n",
      "# of episode :300, avg score : 246.5, buffer size : 15350, epsilon : 6.5%\n",
      "# of episode :320, avg score : 217.6, buffer size : 19701, epsilon : 6.4%\n",
      "# of episode :340, avg score : 189.1, buffer size : 23482, epsilon : 6.3%\n",
      "# of episode :360, avg score : 161.0, buffer size : 26702, epsilon : 6.2%\n",
      "# of episode :380, avg score : 153.2, buffer size : 29765, epsilon : 6.1%\n",
      "# of episode :400, avg score : 149.7, buffer size : 32758, epsilon : 6.0%\n",
      "# of episode :420, avg score : 143.9, buffer size : 35636, epsilon : 5.9%\n",
      "# of episode :440, avg score : 161.8, buffer size : 38873, epsilon : 5.8%\n",
      "# of episode :460, avg score : 175.5, buffer size : 42383, epsilon : 5.7%\n",
      "# of episode :480, avg score : 177.4, buffer size : 45931, epsilon : 5.6%\n",
      "# of episode :500, avg score : 158.8, buffer size : 49107, epsilon : 5.5%\n",
      "# of episode :520, avg score : 139.8, buffer size : 50000, epsilon : 5.4%\n",
      "# of episode :540, avg score : 137.6, buffer size : 50000, epsilon : 5.3%\n",
      "# of episode :560, avg score : 143.6, buffer size : 50000, epsilon : 5.2%\n",
      "# of episode :580, avg score : 140.8, buffer size : 50000, epsilon : 5.1%\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 비디오 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyvirtualdisplay.display.Display at 0x7faeee74ea70>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 결과를 mp4 동영상으로 보여주기 위한 코드\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
    "                loop controls style=\"height: 400px;\">\n",
    "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
    "                </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(1400, 900))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Could not find video\n"
     ]
    }
   ],
   "source": [
    "show_video()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
