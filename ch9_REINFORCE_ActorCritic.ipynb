{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "가장 간단한 Policy gradient algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Policy(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Policy, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(4, 128) # 4 inputs : state\n",
    "        self.fc2 = nn.Linear(128, 2) # 2 outputs : action (left or right)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.softmax(self.fc2(x), dim=0) # softmax : probability, dim=0 : column\n",
    "        return x\n",
    "      \n",
    "    def put_data(self, item):\n",
    "        self.data.append(item)\n",
    "        \n",
    "    def train_net(self):\n",
    "        R = 0\n",
    "        self.optimizer.zero_grad()\n",
    "        for r, prob in self.data[::-1]:\n",
    "            R = r + gamma * R\n",
    "            loss = -torch.log(prob) * R # policy gradient\n",
    "            loss.backward()\n",
    "        self.optimizer.step() # update parameter\n",
    "        self.data = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = gym.make('CartPole-v1')\n",
    "    pi = Policy()\n",
    "    score = 0.0\n",
    "    print_interval = 20\n",
    "    \n",
    "    for n_epi in range(2001):\n",
    "        s, _ = env.reset()\n",
    "        done = False\n",
    "        \n",
    "        while not done: # CartPole-v1 forced to terminates at 500 step.\n",
    "            prob = pi(torch.from_numpy(s).float())\n",
    "            m = Categorical(prob) # discrete probability distributions : allows you to sample from the distribution and compute various properties\n",
    "            a = m.sample() # 하나의 action을 sampling\n",
    "            s_prime, r, done, truncated, info = env.step(a.item())\n",
    "            pi.put_data((r,prob[a])) # network update를 위해 reward와 해당 action의 확률을 저장\n",
    "            s = s_prime\n",
    "            score += r\n",
    "            \n",
    "        pi.train_net() # episode가 끝날 때마다 policy network를 update\n",
    "        \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 21.3\n",
      "# of episode :40, avg score : 20.35\n",
      "# of episode :60, avg score : 25.0\n",
      "# of episode :80, avg score : 25.6\n",
      "# of episode :100, avg score : 26.5\n",
      "# of episode :120, avg score : 24.3\n",
      "# of episode :140, avg score : 25.05\n",
      "# of episode :160, avg score : 24.3\n",
      "# of episode :180, avg score : 31.0\n",
      "# of episode :200, avg score : 32.2\n",
      "# of episode :220, avg score : 31.3\n",
      "# of episode :240, avg score : 27.0\n",
      "# of episode :260, avg score : 29.75\n",
      "# of episode :280, avg score : 40.4\n",
      "# of episode :300, avg score : 37.65\n",
      "# of episode :320, avg score : 34.0\n",
      "# of episode :340, avg score : 39.5\n",
      "# of episode :360, avg score : 38.8\n",
      "# of episode :380, avg score : 36.45\n",
      "# of episode :400, avg score : 34.4\n",
      "# of episode :420, avg score : 39.2\n",
      "# of episode :440, avg score : 53.25\n",
      "# of episode :460, avg score : 45.95\n",
      "# of episode :480, avg score : 47.55\n",
      "# of episode :500, avg score : 43.55\n",
      "# of episode :520, avg score : 44.2\n",
      "# of episode :540, avg score : 41.65\n",
      "# of episode :560, avg score : 42.55\n",
      "# of episode :580, avg score : 52.25\n",
      "# of episode :600, avg score : 47.45\n",
      "# of episode :620, avg score : 57.55\n",
      "# of episode :640, avg score : 70.9\n",
      "# of episode :660, avg score : 52.0\n",
      "# of episode :680, avg score : 68.95\n",
      "# of episode :700, avg score : 63.4\n",
      "# of episode :720, avg score : 58.0\n",
      "# of episode :740, avg score : 61.65\n",
      "# of episode :760, avg score : 62.2\n",
      "# of episode :780, avg score : 74.7\n",
      "# of episode :800, avg score : 82.95\n",
      "# of episode :820, avg score : 95.0\n",
      "# of episode :840, avg score : 82.45\n",
      "# of episode :860, avg score : 90.75\n",
      "# of episode :880, avg score : 85.4\n",
      "# of episode :900, avg score : 96.35\n",
      "# of episode :920, avg score : 90.55\n",
      "# of episode :940, avg score : 111.05\n",
      "# of episode :960, avg score : 112.3\n",
      "# of episode :980, avg score : 95.25\n",
      "# of episode :1000, avg score : 99.45\n",
      "# of episode :1020, avg score : 128.35\n",
      "# of episode :1040, avg score : 122.9\n",
      "# of episode :1060, avg score : 161.05\n",
      "# of episode :1080, avg score : 153.2\n",
      "# of episode :1100, avg score : 169.45\n",
      "# of episode :1120, avg score : 170.5\n",
      "# of episode :1140, avg score : 173.1\n",
      "# of episode :1160, avg score : 206.65\n",
      "# of episode :1180, avg score : 188.15\n",
      "# of episode :1200, avg score : 165.0\n",
      "# of episode :1220, avg score : 147.5\n",
      "# of episode :1240, avg score : 198.45\n",
      "# of episode :1260, avg score : 174.0\n",
      "# of episode :1280, avg score : 213.7\n",
      "# of episode :1300, avg score : 251.3\n",
      "# of episode :1320, avg score : 218.6\n",
      "# of episode :1340, avg score : 204.8\n",
      "# of episode :1360, avg score : 197.6\n",
      "# of episode :1380, avg score : 215.1\n",
      "# of episode :1400, avg score : 216.7\n",
      "# of episode :1420, avg score : 167.1\n",
      "# of episode :1440, avg score : 145.35\n",
      "# of episode :1460, avg score : 197.85\n",
      "# of episode :1480, avg score : 205.65\n",
      "# of episode :1500, avg score : 221.1\n",
      "# of episode :1520, avg score : 177.95\n",
      "# of episode :1540, avg score : 231.0\n",
      "# of episode :1560, avg score : 234.0\n",
      "# of episode :1580, avg score : 214.6\n",
      "# of episode :1600, avg score : 168.9\n",
      "# of episode :1620, avg score : 211.2\n",
      "# of episode :1640, avg score : 225.0\n",
      "# of episode :1660, avg score : 246.35\n",
      "# of episode :1680, avg score : 213.4\n",
      "# of episode :1700, avg score : 211.55\n",
      "# of episode :1720, avg score : 233.75\n",
      "# of episode :1740, avg score : 207.15\n",
      "# of episode :1760, avg score : 260.0\n",
      "# of episode :1780, avg score : 262.1\n",
      "# of episode :1800, avg score : 262.65\n",
      "# of episode :1820, avg score : 224.85\n",
      "# of episode :1840, avg score : 285.75\n",
      "# of episode :1860, avg score : 259.7\n",
      "# of episode :1880, avg score : 287.35\n",
      "# of episode :1900, avg score : 251.75\n",
      "# of episode :1920, avg score : 296.6\n",
      "# of episode :1940, avg score : 288.9\n",
      "# of episode :1960, avg score : 264.4\n",
      "# of episode :1980, avg score : 272.3\n",
      "# of episode :2000, avg score : 272.2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ActorCritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정책 네트워크와 밸류 네트워크를 함께 학습하는 방법론"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Q AC \n",
    "* Advantage AC\n",
    "* TD AC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "learning_rate = 0.0002\n",
    "gamma         = 0.98\n",
    "n_rollout     = 10 # 몇 틱의 데이터를 쌓아서 업데이트할 것인지 -> 10번의 상태 전이를 모아서 업데이트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TDActorCritic(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TDActorCritic, self).__init__()\n",
    "        self.data = []\n",
    "        \n",
    "        self.fc1 = nn.Linear(4,256) # 4 inputs : state, 두 개의 네트워크가 해당 레이어를 공유\n",
    "        self.fc_pi = nn.Linear(256,2) # 2 outputs : action (left or right)\n",
    "        self.fc_v = nn.Linear(256,1) # 1 outputs : value\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "        \n",
    "    def pi(self, x, softmax_dim = 0): # 정책 네트워크 \n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc_pi(x)\n",
    "        prob = F.softmax(x, dim=softmax_dim)\n",
    "        return prob\n",
    "    \n",
    "    def v(self, x): # 밸류 네트워크\n",
    "        x = F.relu(self.fc1(x))\n",
    "        v = self.fc_v(x)\n",
    "        return v\n",
    "    \n",
    "    def put_data(self, transition):\n",
    "        self.data.append(transition)\n",
    "        \n",
    "    def make_batch(self): # mini-batch를 만들어주는 함수\n",
    "        s_lst, a_lst, r_lst, s_prime_lst, done_lst = [], [], [], [], []\n",
    "        for transition in self.data:\n",
    "            s,a,r,s_prime,done = transition\n",
    "            s_lst.append(s)\n",
    "            a_lst.append([a])\n",
    "            r_lst.append([r/100.0])\n",
    "            s_prime_lst.append(s_prime)\n",
    "            done_mask = 0.0 if done else 1.0\n",
    "            done_lst.append([done_mask])\n",
    "        \n",
    "        s_batch, a_batch, r_batch, s_prime_batch, done_batch = torch.tensor(s_lst, dtype=torch.float), torch.tensor(a_lst), \\\n",
    "                                                               torch.tensor(r_lst, dtype=torch.float), torch.tensor(s_prime_lst, dtype=torch.float), \\\n",
    "                                                               torch.tensor(done_lst, dtype=torch.float)\n",
    "        self.data = []\n",
    "        return s_batch, a_batch, r_batch, s_prime_batch, done_batch\n",
    "  \n",
    "    def train_net(self):\n",
    "        s, a, r, s_prime, done = self.make_batch()\n",
    "\n",
    "        # value network\n",
    "        td_target = r + gamma * self.v(s_prime) * done \n",
    "        delta = td_target - self.v(s) # v(s) 의 TD error\n",
    "\n",
    "        # policy network        \n",
    "        pi = self.pi(s, softmax_dim=1) # 이전이랑 tensor의 형태가 달라져서 올바른 함수 적용을 위해 softmax_dim=1로 바꿔줌\n",
    "        pi_a = pi.gather(1,a) # gather(dim, index) : 해당 index의 값들을 모아서 반환\n",
    "\n",
    "        # loss는 policy gradient와 value network의 loss를 합친 것\n",
    "        loss = -torch.log(pi_a) * delta.detach() + F.smooth_l1_loss(self.v(s), td_target.detach()) \n",
    "        # detach: 해당 변수를 상수로 취급 -> 정답은 가만히 있고 예측치가 변하도록 하기 위함\n",
    "        # TD 방식으로 value network의 loss 계산\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.mean().backward() # 10개의 loss의 평균을 구한 후 backpropagation\n",
    "        self.optimizer.step()         \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():  \n",
    "    env = gym.make('CartPole-v1')\n",
    "    model = TDActorCritic()    \n",
    "    print_interval = 20\n",
    "    score = 0.0\n",
    "\n",
    "    for n_epi in range(1001):\n",
    "        done = False\n",
    "        s, _ = env.reset()\n",
    "        while not done:\n",
    "            for t in range(n_rollout):\n",
    "                prob = model.pi(torch.from_numpy(s).float()) # softmax_dim=0\n",
    "                m = Categorical(prob)\n",
    "                a = m.sample().item()\n",
    "                s_prime, r, done, truncated, info = env.step(a)\n",
    "                model.put_data((s,a,r,s_prime,done))\n",
    "                \n",
    "                s = s_prime\n",
    "                score += r\n",
    "                \n",
    "                if done:\n",
    "                    break                     \n",
    "            \n",
    "            model.train_net() # n_rollout 만큼의 데이터를 쌓은 후 학습 진행\n",
    "            \n",
    "        if n_epi%print_interval==0 and n_epi!=0:\n",
    "            print(\"# of episode :{}, avg score : {:.1f}\".format(n_epi, score/print_interval))\n",
    "            score = 0.0\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of episode :20, avg score : 18.5\n",
      "# of episode :40, avg score : 17.8\n",
      "# of episode :60, avg score : 18.1\n",
      "# of episode :80, avg score : 21.1\n",
      "# of episode :100, avg score : 18.8\n",
      "# of episode :120, avg score : 21.1\n",
      "# of episode :140, avg score : 20.1\n",
      "# of episode :160, avg score : 20.6\n",
      "# of episode :180, avg score : 30.1\n",
      "# of episode :200, avg score : 33.2\n",
      "# of episode :220, avg score : 35.0\n",
      "# of episode :240, avg score : 40.7\n",
      "# of episode :260, avg score : 46.5\n",
      "# of episode :280, avg score : 50.8\n",
      "# of episode :300, avg score : 48.1\n",
      "# of episode :320, avg score : 52.7\n",
      "# of episode :340, avg score : 60.2\n",
      "# of episode :360, avg score : 52.0\n",
      "# of episode :380, avg score : 51.4\n",
      "# of episode :400, avg score : 72.5\n",
      "# of episode :420, avg score : 68.5\n",
      "# of episode :440, avg score : 62.2\n",
      "# of episode :460, avg score : 85.3\n",
      "# of episode :480, avg score : 86.2\n",
      "# of episode :500, avg score : 97.6\n",
      "# of episode :520, avg score : 111.8\n",
      "# of episode :540, avg score : 128.8\n",
      "# of episode :560, avg score : 211.1\n",
      "# of episode :580, avg score : 153.6\n",
      "# of episode :600, avg score : 223.3\n",
      "# of episode :620, avg score : 284.1\n",
      "# of episode :640, avg score : 248.9\n",
      "# of episode :660, avg score : 257.8\n",
      "# of episode :680, avg score : 181.6\n",
      "# of episode :700, avg score : 223.7\n",
      "# of episode :720, avg score : 214.6\n",
      "# of episode :740, avg score : 195.8\n",
      "# of episode :760, avg score : 169.9\n",
      "# of episode :780, avg score : 173.3\n",
      "# of episode :800, avg score : 182.3\n",
      "# of episode :820, avg score : 203.3\n",
      "# of episode :840, avg score : 249.6\n",
      "# of episode :860, avg score : 263.9\n",
      "# of episode :880, avg score : 330.5\n",
      "# of episode :900, avg score : 320.1\n",
      "# of episode :920, avg score : 297.4\n",
      "# of episode :940, avg score : 287.6\n",
      "# of episode :960, avg score : 341.3\n",
      "# of episode :980, avg score : 397.1\n",
      "# of episode :1000, avg score : 296.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
